[{"content":" ABOUT ME ","date":null,"permalink":"/","section":"","summary":" ABOUT ME ","title":""},{"content":"About some stuff I like to tinker with. #","date":null,"permalink":"/posts/","section":"","summary":"About some stuff I like to tinker with.","title":""},{"content":"Did you know that World Backup Day is on the 31rst of March? Heck, did you even know there was a World Backup Day? Indeed backing up your data is important, but what to do? Starting by having a remote copy of your most important data on services like Google Drive, Dropbox, Onedrive, etc\u0026hellip; is a good start! But what if you want to do it yourself? Maybe because you want better control over your data, or maybe because you do not want Google to shut down your account because of what you have on your Google Drive ? Or maybe you just want to do it yourself because you can. In any case, the general rule of thumb for backing up data is to follow the 3-2-1 rule:\n3 copies of your important data on 2 different storage mediums 1 of which is off-site The first two points are quite easy to satisfy. However, the last one (having a backup off-site), is a bit more tricky. You either need to be able to access a bit of storage on a friend\u0026rsquo;s NAS or set up a second NAS in your family or friend\u0026rsquo;s house. Certainly doable, but not the easiest solution since you need to maintain two separate computers, one of which is not easily accessible because it is precisely off-site.\nAnother way to back up your data off-site is to use a cloud storage provider and backup software. I chose to use BackBlaze as the cloud provider because it is cheap and easy to use, even if you never used such services before. For the backup software, I will use Duplicati . Duplicati is open source and allows for encrypted and compressed incremental backups. Incremental backup is an incredibly useful feature: You only backup the files that changed since the last backup. You do not need to back up all your files every time! Ah, and yes, Duplicati is compatible with BackBlaze!\nPreparing the cloud ‚òÅÔ∏è #Creating my first bucket ü™£ #Let\u0026rsquo;s tackle the cloud part. Data are stored in a bucket, which is roughly a top-level folder that will contain all my backup data. To create my bucket I first sing up for an account on BackBlaze. I chose the EU region when creating my account because first, I am living in Europe, and then to prevent my data from being stored in the US. I then created a bucket to store my backup data in the \u0026ldquo;buckets\u0026rdquo; section of the \u0026ldquo;My Account\u0026rdquo; page. Beware that it seems like the name should be original among all bucket names on BackBlaze. To make sure the name was not already taken, I added 4 random characters at the end of it. I set the bucket to private. It is possible to enable encryption, however, it is not necessary since my backup will already be encrypted by Duplicati before being uploaded to BackBlaze.\nRegion selection on BackBlaze Bucket Creation on BackBlaze Keys of the realm üîë #To programmatically access your bucket, Duplicati needs API keys - here called Application Keys. If you have never heard of it, you can see them as passwords for applications. I generated keys just for Duplicati in the \u0026ldquo;App Keys\u0026rdquo; section. I gave them a friendly name so I will know what they are used for in the future. Then I chose for which bucket they will be valid. It is possible to select all the buckets, but that is always a good practice to give API keys only the rights they need, nothing more! Then I chose Read and Write rights because I want to upload my backups (write), but I also want to retrieve them if needed (read). I left all the other parameters blank.\nOnce the keys are created they appear in a light blue rectangle (see picture below). I left the webpage open until I copied them in Duplicati. If you follow the same process as I do, beware: the keys only appear once!\nApplication Keys creation Visualisation of the Application Keys Duplicati in a container üê≥ #Getting the container ready üß∞ #Once the cloud part is done, I focused on Duplicati itself. For all the services I run at home, I use Docker. You can get Docker installation instructions here . To keep things simple and readable, I like to use Docker Compose. It can be easily installed with a simple command:\nsudo apt install docker-compose-plugin This command will install docker compose V2 (more info here ). Not the V1 pip package docker-compose (there is a hyphen between docker and compose in this case).\nDocker compose is a nice tool that allows configuring easily docker containers with a text file, which is always called docker-compose.yaml. It also provides some useful tools to recreate containers and pull new images for example.\nI started by creating a new folder called duplicati, and inside this folder, I created a file called, you guessed it, docker-compose.yaml. Examples of docker-compose files are often provided on Docker Hub. It is the case for Duplicati . Here is the file content:\n--- version: \u0026#34;2.1\u0026#34; services: duplicati: image: lscr.io/linuxserver/duplicati:latest container_name: duplicati environment: - PUID=0 - PGID=0 - TZ=Europe/Paris volumes: - ./duplicati-config:/config - ./local-backups:/backups - \u0026lt;path-to-data\u0026gt;:/source ports: - 8200:8200 restart: unless-stopped As you can see, I used the linuxserver.io image.\nPUID and PGID are ids for the user that will run Duplicati inside the container. Generally speaking, you want to set both of them to 1000, which is the id of the first created user. However, I set them to 0, because I want to access files owned by root. TZ is simply the timezone, to make sure scheduled backups run when you expect them to! For the volumes, the config will be stored in duplicati-config and local backups in local-backups. However since I will use BackBlaze, there should not be any backups stored there. Both folders will be inside the duplicati folder since I used relative paths. The path for the /source folder should be updated according to your needs. I map it to an external SSD where are located the files I want to backup. It might be tempting to add the flag ro for read-only, to make sure Duplicati does not alter the source files This will, however, prevent Duplicati from restoring the files in the source destination, and you will have to restore them in another folder (such as ./local-backups). I left the port to the default value of 8200 in this example, but you can use whatever value suits your needs. Once this file was created I started the container with :\ndocker compose up -d The -d option makes sure the container is started in a detached mode. I made sure everything was ok by running docker compose logs -f.\nConfiguration through the web interface üîß #To reach the web interface, I went to the IP of the computer hosting the docker container, followed by the port, so in the case of this example : 8200.\nThe web interface is straightforward. I created a new backup. In destination, I choose specifically the B2 Cloud Storage option. The bucket name is the same as the one I just created : data-backup-ej8c. Folder path is the path inside the bucket. I simply add a directory called backcups at the root of the bucket. and finally I inputed the keyID (for B2 Application ID) and applicationKey (for B2 Application Key) that were generated on the Backblaze website.\nBackup creation in Duplicati I then followed through with the configuration, by selecting my source file and also specifying the backup schedule.\nOnce the backup is done, I clicked Run Now to make sure the first backup was created and voil√†!\nBrowsing the files on Backblaze should now show a bunch of files all of the same sizes (except for a few with index in the name).\nBrowsing the files in the bucket Updating Duplicati ‚¨ÜÔ∏è #Since I used Docker Compose, updating Duplicati is really simple. Once in the same folder as the docker-compose.yaml file, I first update the image with :\ndocker compose pull and recreate the container with:\ndocker compose up -d Reliable backups ‚òùÔ∏è #Backblaze can arguably be regarded as reliable. It is to say that I will not lose files hosted there. But what if I lost the Duplicati container? Recreating the container can be achieved in a matter of minutes from a fresh docker-compose.yaml file. However, I will miss the backup configuration. Fortunately, it is possible to export a Duplicati backup configuration file. I exported the configuration file with the passwords (it is to say with the Backblaze API keys) and safely backed it up on another computer.\n‚ö†Ô∏è Bear in mind that if you encrypted your backup, the recovery key will not be exported in the configuration file and you will have to back it up yourself somewhere. Maybe on your password manager?\n","date":"9 October 2022","permalink":"/posts/data-backup/","section":"","summary":"Did you know that World Backup Day is on the 31rst of March?","title":"üíæ DIY Cloud backup"},{"content":"Recently I wanted to play Pok√©mon Fire Red üî• on original hardware, it is to say a Game Boy Advance (GBA) and not an emulator of any kind. Since I had a GBA SP (AGS-001) as a kid, I wanted to play on a standard GBA for the sake of \u0026ldquo;novelty\u0026rdquo;. However, the original GBA has a reflective screen. This type of screen does not emit its own light and needs an external light source (such as the sun for example). This makes the screen notoriously difficult to see since you need to orient it in a certain angle in order to see what it is displaying. More modern screen technologies include their own source of light. The first generation of GameBoy Advance SP (AGS-001) model features a frontlit screen. This is the same principle as the old reflective screen, except that light is emitted by a small led strip at the bottom of the screen. The latest GBA SP model (AGS-101) features a modern backlit screen, where the light source comes from behind the pixels. However, this GBA SP revision is rarer that the common AGS-001 frontlit model. In order to play in the best conditions but yet with original hardware, I will mod an original GBA to make a more up-to-date handheld console.\nOriginal GBA Modded GBA Modding a GBA #As I told you, the main improvement point for the GBA is to replace the screen for a modern backlit LCD. However this is not the only thing you might want to change. Here is a list of all the parts I bought for my modding project. I ordered all my parts on Retro Moding , although you can buy them on various specialized websites as well Aliexpress.\nParts # IPS LCD Screen : There are various LCD mods for the GBA, such as retrofitting a AGS-101 (second GBA SP version) screen, or a DSi lower screen (this mod is often called ITA : DSi to GBA ). However I went with the more modern FunnyPlaying IPS LCD V2, which supports brightness control provided that you solder a few wires (more on that later).\nNew shell : I decided to buy a new shell for 3 reasons. First, because I bought a second-hand Game Boy Advance and although the shell was in pretty good condition, I vanted to have a brand new shell feeling. Then because the LCD screen I bought necessitates some cuts in the original shell, and I wanted to preserve it. Fortunately FunnyPlaying also provides shell specifically moulded for their LCD. And finally, because there are a lot of color to choose from. So you can build yourself a nice-looking GBA!\nNew Buttons : Just to match your new shell!\nNew Silicone Pads : The original silicone pads might be a bit worn out so you might want to replace them. Moreover, if you want to use a clear case, you can choose colorful silicon pads. Bear in mind that your start and select buttons will the same color as the silicone pads (since they actually are a one-piece silicone pad)!\nNew Glass Lens : The FunnyPlaying IPS LCD V2 has slightly different dimensions from the old screen, so you need an appropriate lens. A glass lens will be more scratch-resistant than a plastic one, but it might shatter if dropped. Pick the one you like! There are several colors and variants as well.\nUSB-C Rechargeable Battery : The original Game Boy Advance used two AA batteries. Fortunately, you can now use a USB-C rechargeable battery pack that fits in the battery compartment. This battery pack fits in the FunnyPlaying shell without modification. If you use an original shell, you will need to trim a bit the battery compartment.\nBuilding Process #Disassembly #First I started by fully dissasembling the original GBA. For this step you need two screw drivers. A standard philips screw driver and a tri-wing one (this type of screw is very oftenly used by Nintendo). This is an appropriate time to clean the PCB with isopropyl alcohol, especially the golden pads located under the buttons to make sure the silicon pads make good contact with them.\nView of all the parts GBA motherboard Original GBA Close up of the motherboard Although I did not take any pictures of this step, I advise you to test the screen at this step. Simply put the ribon cable of the new screen in the connector located at the top of the motherboard, and power up your GBA, just to confirm the screen is turning on and showing the Game Boy logo.\nAssembly #Once I confirmed the screen was ok, I soldered the three tiny wires that came with the screen. These wires are to be soldered on the motherboard to enable screen brightness control through select+L or select+R. I found easier to solder the wires to the ribon cable first and then to solder the other ends to the appropriate locations. I used a bit of duct tape (since I did not had any kapton tape) to secure the ribon cable in place on the back of the screen.\nClose up of the soldered wires Close up of the motherboard I placed the screen in the case (be gentle, you do not want to break the LCD pannel) and solder the wires to the motherboard. I used the foam pad that came with the screen. I made a small inscision in the foam pad to let the wires through. Do not forget to put the buttons and the silicon pads as well !\nDepending on your motherboard revision, there are two possible screen connector. One with 32 pins and one with 40 pins. Mine was 40 pins, but if you have the 32 pins variant, just bend the ribon cable to expose the correct connector on the top of the screen.\nWires soldered to the motherboard After that, I secured the motherboard to the front part of the shell. I then added the battery cover and the sticker for a more genuine look.\nWith the back of the shell Battery cover and sticker added Ready to add the glass lens Before putting the glass lens, I made sure that there were no dust on the screen or the screen-facing side of the glass lens, since it would not be possible to clean it later. I finally removed the glass lens protection film. Here is the final result :\nWith the back of the shell Now let\u0026rsquo;s beat the Pok√©mon league !\n","date":"5 April 2022","permalink":"/posts/game-boy-modding/","section":"","summary":"Recently I wanted to play Pok√©mon Fire Red üî• on original hardware, it is to say a Game Boy Advance (GBA) and not an emulator of any kind.","title":"üõ†Ô∏è Modding your Game Boy Advance"},{"content":"About Ethereum and Ethereum mining #Ethereum is the second most popular cryptocurrency by volume on the internet. One nice thing about it is that it has been conceived to be ASICs-proof. You can in theory, only mine ETH on a GPU or CPU. It is to say that you could help secure the blockchain and more importantly earn a bit of ETH from your computer. No need for an expensive and noisy ASIC miner !\nIf you want to mine cryptocurrency easily, you can use premade binaries (such as T-Rex miner ) or services (such as NiceHash ). However, bear in mind that most of these binaries or services take a small fee, generally around 1% of your hard-earned ETH. If you would rather keep this 1% for you or use open-source software, this is possible thanks to ethminer !\nEthminer is an open-source Ethereum miner written in C++ and compatible with both AMD (through OpenCL) and Nvidia (through CUDA) GPUs. The latest release of ethminer is from july 2019, and is built against CUDA 9 at best. With CUDA 9 you will not be able to run ethminer on the most recent Nvidia cards (RTX 3000 series, for example, requires at least CUDA 11.1). Therefore you will need to build ethminer yourself if you want to use recent GPUs. This is far from impossible but a bit tedious since ethminer uses Hunter to fetch some dependencies. Hunter uses Bintray, which has been sunseted on the 1rst of May 2021.\nTo keep this simple, the easiest way to use ethminer is to use a docker container that does all the hard work of building and running ethminer for you. If you just want to use the Docker image directly, jump to the section about actually running ethminer. If you want some details about how to make the Dockerfile follow through the next section!\nRequirements #For this project you will need to have :\nA Linux machine with at least one Nvidia GPU. Docker. It can be installed on linux with curl https://get.docker.com | sh nvidia-docker. It can be installed on Debian based version of Linux with sudo apt install nvidia-docker2 The Nvidia drivers. It can be installed on Ubuntu with sudo apt install nvidia-headless-470-server if it was not already installed. Creating the Dockerfile #First, let us create a directory to work in, I will call it ethminer-docker. In our folder, we will first create a script to launch ethminer. Create a file mining.sh and write the following script:\nethminer --HWMON 2 \\ -P $MINING_ADDRESS \\ --api-bind 0.0.0.0:3333 --HWMON 2 Enable hardware monitoring -P $MINING_ADDRESS Uses an environment variable to store the pool address + wallet --api-bind 0.0.0.0:3333 Enable the API on port 3333. There is no need to change the port, since we can expose a different port to the host with Docker.\nYour folder structure should now look like this:\nethminer-docker ‚îî‚îÄ‚îÄ mining.sh Now, in order to build our Docker container, we are going to write a file called Dockerfile. This file will contain all the instruction to create the docker image. The original instructions specific to ethminer can be found on the docs/BUILD.md file of the ethermine repo.\nIn our Dockerfile, we first start by writing the following line that allows us to use a premade image from Nvidia containing the drivers as a base image, on top of which we will install the drivers. As you can see here, our docker image will have the driver version 460.73.01.\nFROM nvidia/driver:460.73.01-ubuntu20.04 Then we update the sources and install the necessary dependencies to build ethminer.\nThe environment variable DEBIAN_FRONTEND is here to prevent apt-get from asking us questions since the installation process is supposed to be unattended. That is also why we add -y : to accept without further input from apt-get\nRUN apt-get update \u0026amp;\u0026amp; \\ DEBIAN_FRONTEND=noninteractive apt-get install kmod git cmake perl gcc g++ wget --no-install-recommends -yq We fetch the CUDA install script, run it and finaly delete it in the same RUN statement. This prevents the addition of a lot of useless layer in the creation of the docker image. Here we download CUDA version 11.4.2\nRUN wget --no-check-certificate https://developer.download.nvidia.com/compute/cuda/11.4.2/local_installers/cuda_11.4.2_470.57.02_linux.run \u0026amp;\u0026amp; \\ sh cuda_11.4.2_470.57.02_linux.run --silent --toolkit --no-man-page --no-opengl-libs \u0026amp;\u0026amp; \\ rm cuda_11.4.2_470.57.02_linux.run We change our working directory to /.\nWORKDIR \u0026#34;/\u0026#34; This big one-liner is responsible for cloning, configuring, building ethminer and finaly removing the useless files. This command is quite a big chunk so I will explained it in details. I used a big one-liner once again to limit the number of layers generated.\nRUN git clone https://github.com/ethereum-mining/ethminer.git -o ethminer \u0026amp;\u0026amp; \\ cd /ethminer \u0026amp;\u0026amp; \\ git submodule update --init --recursive \u0026amp;\u0026amp; \\ mkdir build \u0026amp;\u0026amp; cd build \u0026amp;\u0026amp; \\ # Hack because bintray does not exists anymore # see https://unix.stackexchange.com/questions/652841/boost-continually-fails-to-download-while-using-cmake-for-ethminer sed -i \u0026#39;/hunter_config(Boost VERSION 1.66.0)/c\\hunter_config(\\n Boost\\n VERSION 1.66.0_new_url\\n SHA1 f0b20d2d9f64041e8e7450600de0267244649766\\n URL https://boostorg.jfrog.io/artifactory/main/release/1.66.0/source/boost_1_66_0.tar.gz\\ )\u0026#39; /ethminer/cmake/Hunter/config.cmake \u0026amp;\u0026amp; \\ cmake .. -DETHASHCL=OFF -DAPICORE=ON -DETHASHCUDA=ON -DBINKERN=OFF \u0026amp;\u0026amp; \\ cmake --build . \u0026amp;\u0026amp; \\ make install \u0026amp;\u0026amp; \\ cd / \u0026amp;\u0026amp; rm -rf ethminer In this command, we first clone the repo on a folder called ethminer. We then basically follow the building instruction from the ethminer repo. But before the configuring, we use a big sed command to edit the Hunter configuration. This is necessary since without this modification, Hunter is going to try to fetch Boost on Bintray and fail. Downloading manually Boost is not going to work either. The hash does not correspond to the one expected by Hunter, hence this modification with sed. More details here .\nOnce the modification of the Hunter config.cmake file is done, we generate the build configuration with 4 flags. -DETHASHCL=OFF disable OpenCL, since it is for AMD GPUs. However, ethminer can mine on both AMD and Nvidia GPUs at the same time. So if you want to mine Ethereum on such a hardware configuration, enable OpenCL with -DETHASHCL=ON. -DAPICORE=ON Enables the API (more details on that later). -DETHASHCUDA=ON makes sure CUDA support is enabled. And finally -DBINKERN=OFF prevent the installation of AMD binary kernels (once again, enable it if you want to use AMD GPUs).\nOnce this is done, we build ethminer, then install it (so that it is in our $PATH). and finally delete the repo directory as we do not need any of these files anymore.\\\nNow we expose port 3333 (or any other port that you want to use, but make sure that this is the same port as the one in your mining.sh script) to access the API. The API is a simple web page with a few statistics such as the list of the GPUs, their temperature, fan speed and hash rate. This is helpful since we can easily access all these informations without having to look at the docker logs.\nEXPOSE 3333 Example of the ethminer API :\nExample of ethminer API dashboard We need to copy our mining script in the / directory of you docker image.\nCOPY mining.sh . Finally, we have to overrive our base image entrypoint by adding a new one which will launch our mining script!\nENTRYPOINT [ \u0026#34;bash\u0026#34;, \u0026#34;mining.sh\u0026#34; ] The final Dockerfile is named Dockerfile (no extension) and contains the following :\nFROM nvidia/driver:460.73.01-ubuntu20.04 RUN apt-get update \u0026amp;\u0026amp; \\ DEBIAN_FRONTEND=noninteractive apt-get install kmod git cmake perl gcc g++ wget --no-install-recommends -yq RUN wget --no-check-certificate https://developer.download.nvidia.com/compute/cuda/11.4.2/local_installers/cuda_11.4.2_470.57.02_linux.run \u0026amp;\u0026amp; \\ sh cuda_11.4.2_470.57.02_linux.run --silent --toolkit --no-man-page --no-opengl-libs \u0026amp;\u0026amp; \\ rm cuda_11.4.2_470.57.02_linux.run WORKDIR \u0026#34;/\u0026#34; RUN git clone https://github.com/ethereum-mining/ethminer.git -o ethminer \u0026amp;\u0026amp; \\ cd /ethminer \u0026amp;\u0026amp; \\ git submodule update --init --recursive \u0026amp;\u0026amp; \\ mkdir build \u0026amp;\u0026amp; cd build \u0026amp;\u0026amp; \\ sed -i \u0026#39;/hunter_config(Boost VERSION 1.66.0)/c\\hunter_config(\\n Boost\\n VERSION 1.66.0_new_url\\n SHA1 f0b20d2d9f64041e8e7450600de0267244649766\\n URL https://boostorg.jfrog.io/artifactory/main/release/1.66.0/source/boost_1_66_0.tar.gz\\ )\u0026#39; /ethminer/cmake/Hunter/config.cmake \u0026amp;\u0026amp; \\ cmake .. -DETHASHCL=OFF -DAPICORE=ON -DETHASHCUDA=ON -DBINKERN=OFF \u0026amp;\u0026amp; \\ cmake --build . \u0026amp;\u0026amp; \\ make install \u0026amp;\u0026amp; \\ cd / \u0026amp;\u0026amp; rm -rf ethminer EXPOSE 3333 COPY mining.sh . ENTRYPOINT [ \u0026#34;bash\u0026#34;, \u0026#34;mining.sh\u0026#34; ] Your folder structure should now look like this:\nethminer-docker ‚îú‚îÄ‚îÄ Dockerfile ‚îî‚îÄ‚îÄ mining.sh Building our Docker image #Once we have our Dockerfile, this is a rather easy step. Assuming that you are in a directory containing only your Dockerfile, run\ndocker build -t ethminer . ‚ö†Ô∏è Do not forget the dot at the end of the command (it means \u0026ldquo;the current directory\u0026rdquo;) !\nYou can change ethermine by what you want. It is the name of your docker image. You can confirm your docker image is on your system by running docker image ls. This should give something like this :\nREPOSITORY TAG IMAGE ID CREATED SIZE ethminer latest bcf676a57879 6 hours ago 7.15GB Running ethminer on docker #Before running ethminer, you need to install nvidia-docker. This is a wrapper that allows docker to access your GPUs ! To install it, run :\nsudo apt install nvidia-docker Once this is done, you can run the container with :\ndocker run -p \u0026lt;port\u0026gt;:3333 -d -e MINING_ADDRESS=\u0026lt;your-mining-address\u0026gt; --name my_ethminer ‚ö†Ô∏è Replace \u0026lt;port\u0026gt; by the port you want to use and \u0026lt;your-mining-address\u0026gt; by the address of the pool, combined with your wallet. You can find more info about how to write the mining address here The -p \u0026lt;port\u0026gt;:3333 option allows to acces the ethminer API on port \u0026lt;port\u0026gt; of the machine you are currently running it on. If you are happy with port 3333, you can simply use -p 3333 instead. --name my_ethminer gives a friendly name to your container. The -d makes the docker container run in detached mode. If you want to see the logs run :\ndocker logs -f my_ethminer You should see logs like this :\nethminer 0.19.0 Build: linux/release/gnu i 16:39:15 ethminer Configured pool \u0026lt;pool-address\u0026gt; i 16:39:15 ethminer Api server listening on port 3333. i 16:39:15 ethminer Selected pool \u0026lt;pool-address\u0026gt; i 16:39:15 ethminer Stratum mode : Eth-Proxy compatible i 16:39:15 ethminer Established connection to \u0026lt;pool-address\u0026gt; [\u0026lt;pool-ip\u0026gt;] i 16:39:15 ethminer Spinning up miners... cu 16:39:15 cuda-0 Using Pci Id : 04:00.0 NVIDIA GeForce RTX 3070 (Compute 8.6) Memory : 7.65 GB cu 16:39:15 cuda-1 Using Pci Id : 05:00.0 NVIDIA GeForce GTX 1060 6GB (Compute 6.1) Memory : 7.65 GB i 16:39:15 ethminer Epoch : 448 Difficulty : 4.29 Gh i 16:39:15 ethminer Job: deaebb22‚Ä¶ block 13468527 \u0026lt;pool-address\u0026gt; [\u0026lt;pool-ip\u0026gt;] i 16:39:17 ethminer Job: 0ca80852‚Ä¶ block 13468527 \u0026lt;pool-address\u0026gt; [\u0026lt;pool-ip\u0026gt;] cu 16:39:17 cuda-1 Generating DAG + Light(on GPU) : 4.57 GB cu 16:39:17 cuda-0 Generating DAG + Light(on GPU) : 4.57 GB i 16:39:17 ethminer New API session from \u0026lt;your-local-ip+port\u0026gt; i 16:39:17 ethminer New API session from \u0026lt;your-local-ip+port\u0026gt; i 16:39:19 ethminer Job: e0a946a7‚Ä¶ block 13468527 \u0026lt;pool-address\u0026gt; [\u0026lt;pool-ip\u0026gt;] m 16:39:20 ethminer 0:00 A0 0.00 h - cu0 0.00 62C 30% 188.24W, cu1 0.00 57C 49% 51.91W If it is the case, congratulation! You are now mining Ethereum on your computer thanks to Docker! There was no need to install kmod, git, cmake, perl, etc\u0026hellip; or even the CUDA Toolkit. Everything was done while building the docker image!\nIf you want to stop the container, simply run docker stop my_ethminer and docker start my_ethminer to start it back.\n","date":"21 October 2021","permalink":"/posts/ethereum-mining/","section":"","summary":"About Ethereum and Ethereum mining #Ethereum is the second most popular cryptocurrency by volume on the internet.","title":"üêã Custom Docker image for Ethereum mining"},{"content":" I graduated from √âcole des Mines de Saint-Etienne and Imperial College London in 2017. At the Imperial College London, I realized my master thesis on the detection of sources propagating rumours in social networks under the supervision of Prof. Pier Luigi Dragotti .\nI worked at Air France Operational Research from 2018 to 2019, doing predictive maintenance on the A380 and A320 fleets.\nIn december 2023 I defended my PhD, supervised by Hichem Sahbi at Sorbonne University, in the MLIA and then SYEL team of the LIP6. This PhD is realized in partnership with the LIP6 and Netatmo, a company designing smart and connected products for the house. My current research focuses on neural network compression, pruning and sparsity in general.\nPublication and Academia #PhD Manuscript ‚Äì Sorbonne Universit√©, 12/2023 Deep Neural Network Compression for Visual Recognition. PDF SLIDES (üá´üá∑) Extracting Effective Subnetworks with Gumbel-Softmax - ICIP 2022, Oral Robin Dupont, Mohammed Amine Alaoui, Hichem Sahbi, Alice Lebois ARXIV SLIDES POSTER VIDEO Weight Reparametrization for Budget-Aware Network Pruning - ICIP 2021 Robin Dupont, Hichem Sahbi, Guillaume Michel ARXIV SLIDES POSTER VIDEO Master Thesis - Imperial College London, 2017 I studied rumours propagation in social networks and social graph by modelling rumour spreading and creating a maximum likelihood detection algorithm for rumour sources. This master thesis was made under the supervision of Prof. Pier Luigi Dragotti. PDF POSTER Social Network Analysis Project - University of Edinburgh, 2015 I built algorithms for automatic extraction and analysis of social networks taken from literary masterpieces. PDF Miscellaneous #I like DIY, and here are a few things I am proud of:\nI built my own mechanical keyboard running the open-source firmware qmk , my own electric guitar and magic mirror , among other things I retro-engineered the local API of my connected speaker and created a python library for anybody to use, available on github and PyPi I bypassed my ISP box to use my own router and setup several VLANs to isolate IoT devices from trusted devices at home I run several useful services at home such as Nextcloud , Home Assistant , AdGuard or Jellyfin , all in docker containers I created a bot called Turbodose , which sends notifications on your smartphone when there is an available appointment nearby to get vaccinated (Covid19) ","date":null,"permalink":"/about/","section":"","summary":"I graduated from √âcole des Mines de Saint-Etienne and Imperial College London in 2017.","title":"About me"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"}]